{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare batch scoring model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will prepare a model used to detect suspicious activity that will be used for batch scoring.\n",
        "\n",
        "The team at Woodgrove Bank has provided you with exported CSV copies of historical data for you to train your model against. Run the following cell to load required libraries and download the data sets from the Azure ML datastore."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade azureml-train-automl-runtime==1.36.0\r\n",
        "#!pip install --upgrade azureml-automl-runtime==1.36.0\r\n",
        "#!pip install --upgrade scikit-learn\r\n",
        "#!pip install --upgrade numpy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Environment, Datastore, Dataset\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.run import Run\n",
        "from azureml.core.model import Model\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# sklearn.externals.joblib was deprecated in 0.21\n",
        "from sklearn import __version__ as sklearnver\n",
        "from packaging.version import Version\n",
        "if Version(sklearnver) < Version(\"0.21.0\"):\n",
        "    from sklearn.externals import joblib\n",
        "else:\n",
        "    import joblib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Load data\n",
        "ds = Datastore.get(ws, \"woodgrovestorage\")\n",
        "\n",
        "account_ds = Dataset.Tabular.from_delimited_files(path = [(ds, 'synapse/Account_Info.csv')])\n",
        "fraud_ds = Dataset.Tabular.from_delimited_files(path = [(ds, 'synapse/Fraud_Transactions.csv')])\n",
        "untagged_ds = Dataset.Tabular.from_delimited_files(path = [(ds, 'synapse/Untagged_Transactions.csv')])\n",
        "\n",
        "# Create pandas dataframes from datasets\n",
        "account_df = account_ds.to_pandas_dataframe()\n",
        "fraud_df = fraud_ds.to_pandas_dataframe()\n",
        "untagged_df = untagged_ds.to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411416397
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sklearnver)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411416582
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import __version__ as amlver\n",
        "print(amlver)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "gather": {
          "logged": 1613413226431
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411417403
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### Reorder the column of dataframe by ascending order in pandas \n",
        "cols=untagged_df.columns.tolist()\n",
        "cols.sort()\n",
        "untagged_df=untagged_df[cols]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411417669
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare accounts\n",
        "\n",
        "Remove columns that have very few or no values: `accountOwnerName`, `accountAddress`, `accountCity` and `accountOpenDate` "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean = account_df[[\"accountID\", \"transactionDate\", \"transactionTime\", \n",
        "                               \"accountPostalCode\", \"accountState\", \"accountCountry\", \n",
        "                               \"accountAge\", \"isUserRegistered\", \"paymentInstrumentAgeInAccount\", \n",
        "                               \"numPaymentRejects1dPerUser\"]]\n",
        "account_df_clean = account_df_clean.copy()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411417843
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleanup `paymentInstrumentAgeInAccount`. Values that are not numeric, are converted to NaN and then we can fill those NaN values with 0."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean['paymentInstrumentAgeInAccount'] = pd.to_numeric(account_df_clean['paymentInstrumentAgeInAccount'], errors='coerce')\n",
        "account_df_clean['paymentInstrumentAgeInAccount'] = account_df_clean[['paymentInstrumentAgeInAccount']].fillna(0)['paymentInstrumentAgeInAccount']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411417989
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's convert the `numPaymentRejects1dPerUser` so that the column has a datatype of `float` instead of `object`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean[\"numPaymentRejects1dPerUser\"] = account_df_clean[[\"numPaymentRejects1dPerUser\"]].astype(float)[\"numPaymentRejects1dPerUser\"]\n",
        "account_df_clean[\"numPaymentRejects1dPerUser\"].value_counts()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411418157
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to combine the `transactionDate` and `transactionTime` fields into a single field `transactionDateTime`. Begin by converting the transactionTime from an integer to a 0 padded six digit string of the format hhmmss (2 digit hour minute second), then concatenate the two columns and finally parse the concatenated string as a DateTime value."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean[\"transactionTime\"] = ['{0:06d}'.format(x) for x in account_df_clean[\"transactionTime\"]]\n",
        "account_df_clean[\"transactionDateTime\"] = pd.to_datetime(account_df_clean[\"transactionDate\"].map(str) + account_df_clean[\"transactionTime\"], format='%Y%m%d%H%M%S')\n",
        "account_df_clean[\"transactionDateTime\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411418595
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411418749
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`account_df_clean` is now ready for use in modeling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare untagged transactions\n",
        "\n",
        "There are 16 columns in the untagged_transactions whose values are all null. Drop these columns to simplify the dataset. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean = untagged_df.dropna(axis=1, how=\"all\").copy()\n",
        "untagged_df_clean.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411419020
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace null values in `localHour` with `-99`. Also replace values of `-1` with `-99`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean[\"localHour\"] = untagged_df_clean[\"localHour\"].fillna(-99)\n",
        "untagged_df_clean.loc[untagged_df_clean.loc[:,\"localHour\"] == -1, \"localHour\"] = -99\n",
        "untagged_df_clean[\"localHour\"].value_counts()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411419170
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean up the remaining null fields:\n",
        "- Fix missing values for location fields by setting them to `NA` for unknown. \n",
        "- Set `isProxyIP` to False\n",
        "- Set `cardType` to `U` for unknown (which is a new level)\n",
        "- Set `cvvVerifyResult` to `N` which means for those where the transaction failed because the wrong CVV2 number was entered ro no CVV2 numebr was entered, treat those as if there was no CVV2 match."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean = untagged_df_clean.fillna(value={\"ipState\": \"NA\", \"ipPostcode\": \"NA\", \"ipCountryCode\": \"NA\", \n",
        "                               \"isProxyIP\":False, \"cardType\": \"U\", \n",
        "                               \"paymentBillingPostalCode\" : \"NA\", \"paymentBillingState\":\"NA\",\n",
        "                               \"paymentBillingCountryCode\" : \"NA\", \"cvvVerifyResult\": \"N\"\n",
        "                              })"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411419346
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `transactionScenario` column provides no insights because all rows have the same `A` value. Drop that column. Same idea for the `transactionType` column."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "del untagged_df_clean[\"transactionScenario\"]\n",
        "del untagged_df_clean[\"transactionType\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411419570
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the `transactionDateTime` in the same way as shown previously."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean[\"transactionTime\"] = ['{0:06d}'.format(x) for x in untagged_df_clean[\"transactionTime\"]]\n",
        "untagged_df_clean[\"transactionDateTime\"] = pd.to_datetime(untagged_df_clean[\"transactionDate\"].map(str) + untagged_df_clean[\"transactionTime\"], format='%Y%m%d%H%M%S')\n",
        "untagged_df_clean[\"transactionDateTime\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411420180
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`untagged_df_clean` is now ready for use in modeling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare fraud transactions\n",
        "\n",
        "The `transactionDeviceId` has no meaningful values, so drop it. Also, fill NA values of the `localHour` field with -99 as we did for the untagged transactions."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df_clean = fraud_df.copy()\n",
        "del fraud_df_clean['transactionDeviceId']\n",
        "fraud_df_clean[\"localHour\"] = fraud_df_clean[\"localHour\"].fillna(-99)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411420343
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, add the transactionDateTime column to the fraud data set using the same approach that was used for the untagged dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df_clean[\"transactionTime\"] = ['{0:06d}'.format(x) for x in fraud_df_clean[\"transactionTime\"]]\n",
        "fraud_df_clean[\"transactionDateTime\"] = pd.to_datetime(fraud_df_clean[\"transactionDate\"].map(str) + fraud_df_clean[\"transactionTime\"], format='%Y%m%d%H%M%S')\n",
        "fraud_df_clean[\"transactionDateTime\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411420504
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df_clean.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411420649
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, remove any duplicate rows from the fraud data set. We identify a unique transaction by the features `transactionID`, `accountID`, `transactionDateTime` and `transactionAmount`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df_clean = fraud_df_clean.drop_duplicates(subset=['transactionID', 'accountID', 'transactionDateTime', 'transactionAmount'], keep='first')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411420785
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`fraud_df_clean` is now ready for use in modeling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Enrich the untagged data with account data\n",
        "\n",
        "In this section, you will join the untagged dataset with the account dataset to enrich each untagged example."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "latestTrans_df = pd.merge(untagged_df_clean, account_df_clean, on='accountID', suffixes=('_unt','_act'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411421129
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latestTrans_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411421451
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latestTrans_df = latestTrans_df[latestTrans_df['transactionDateTime_act'] <= latestTrans_df['transactionDateTime_unt']]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411421688
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the latest record timestamp."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "latestTrans_df = latestTrans_df.groupby(['accountID','transactionDateTime_unt']).agg({'transactionDateTime_act':'max'})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411422178
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latestTrans_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411422330
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join the latest transactions with the untagged data frame and then the account data frame."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = pd.merge(untagged_df_clean, latestTrans_df, how='outer', left_on=['accountID','transactionDateTime'], right_on=['accountID','transactionDateTime_unt'])\n",
        "joined_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411422846
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df = pd.merge(joined_df, account_df_clean, left_on=['accountID','transactionDateTime_act'], right_on=['accountID','transactionDateTime'])\n",
        "joined_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411423321
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411423573
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick out only the columns needed for the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_join_acct_df = joined_df[['transactionID', 'accountID', 'transactionAmountUSD', 'transactionAmount','transactionCurrencyCode', 'localHour',\n",
        "          'transactionIPaddress','ipState','ipPostcode','ipCountryCode', 'isProxyIP', 'browserLanguage','paymentInstrumentType',\n",
        "           'cardType', 'paymentBillingPostalCode', 'paymentBillingState', 'paymentBillingCountryCode', 'cvvVerifyResult',\n",
        "           'digitalItemCount', 'physicalItemCount', 'accountPostalCode', 'accountState', 'accountCountry', 'accountAge',\n",
        "           'isUserRegistered', 'paymentInstrumentAgeInAccount', 'numPaymentRejects1dPerUser', 'transactionDateTime_act'\n",
        "          ]]\n",
        "untagged_join_acct_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411423846
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rename the columns to clean the names up and remove the suffixes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_join_acct_df = untagged_join_acct_df.rename(columns={\n",
        "                                      'transactionDateTime_act':'transactionDateTime'\n",
        "                                     })"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411423987
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_join_acct_df.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411424222
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Labeling fraud examples\n",
        "\n",
        "First, get the fraud time period for each account. Do this by grouping the fraud data by `accountID`. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_t2 = fraud_df_clean.groupby(['accountID']).agg({'transactionDateTime':['min','max']})"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411424364
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Give these new columns some more friendly names."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_t2.columns = [\"_\".join(x) for x in fraud_t2.columns.ravel()]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411424509
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_t2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411424658
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now left join the untagged dataset with the fraud dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_joinedto_ranges = pd.merge(untagged_join_acct_df, fraud_t2, on='accountID', how='left')\n",
        "untagged_joinedto_ranges"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411424866
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use the joined data to apply a label according to the following rules:\n",
        "* accountID from untagged not found in fraud dataset at all tagged as 0, meaning not fraudulent.\n",
        "* accountID from untagged found in fraud dataset, but the transactionDateTime is outside of the time range from the fraud dataset tagged as 2.\n",
        "* accountID from untagged found in fraud dataset and the transactionDateTime is within the time range from the fraud dataset tagged as 1, meaning fraudulent. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "def label_fraud_range(row):\n",
        "    if (str(row['transactionDateTime_min']) != \"NaT\") and (row['transactionDateTime'] >= row['transactionDateTime_min']) and (row['transactionDateTime'] <= row['transactionDateTime_max']):\n",
        "        return 1\n",
        "    elif (str(row['transactionDateTime_min']) != \"NaT\") and row['transactionDateTime'] < row['transactionDateTime_min']:\n",
        "        return 2\n",
        "    elif (str(row['transactionDateTime_max']) != \"NaT\") and row['transactionDateTime'] > row['transactionDateTime_max']:\n",
        "        return 2\n",
        "    else:\n",
        "        return 0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411425001
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_df_clean = untagged_joinedto_ranges\n",
        "tagged_df_clean['label'] = untagged_joinedto_ranges.apply(lambda row: label_fraud_range(row), axis=1)\n",
        "tagged_df_clean"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411435940
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This leaves us with 1,170 fraudulent examples, 198,326 non-fraudulent examples, and 504 examples that we will ignore as having occured prior to or after the fraud."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_df_clean['label'].value_counts()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411436068
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove those examples with label value of 2 and drop the features `transactionDateTime_min` and `transactionDateTime_max`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_df_clean = tagged_df_clean[tagged_df_clean['label'] != 2]\n",
        "del tagged_df_clean['transactionDateTime_min']\n",
        "del tagged_df_clean['transactionDateTime_max']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411436252
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_df_clean.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411436400
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode the transformations into custom transformers for use in a pipeline as follows:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class NumericCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self = self\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"NumericCleaner.fit called\")\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"NumericCleaner.transform called\")\n",
        "        X[\"localHour\"] = X[\"localHour\"].fillna(-99)\n",
        "        X[\"accountAge\"] = X[\"accountAge\"].fillna(-1)\n",
        "        X[\"numPaymentRejects1dPerUser\"] = X[\"numPaymentRejects1dPerUser\"].fillna(-1)\n",
        "        X.loc[X.loc[:,\"localHour\"] == -1, \"localHour\"] = -99\n",
        "        return X\n",
        "\n",
        "class CategoricalCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self = self\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"CategoricalCleaner.fit called\")\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"CategoricalCleaner.transform called\")\n",
        "        X = X.fillna(value={\"cardType\":\"U\",\"cvvVerifyResult\": \"N\"})\n",
        "        X['isUserRegistered'] = X.apply(lambda row: 1 if row[\"isUserRegistered\"] == \"TRUE\" else 0, axis=1)\n",
        "        return X"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411436564
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "numeric_features=[\"transactionAmountUSD\", \"localHour\", \n",
        "                  \"transactionIPaddress\", \"digitalItemCount\", \"physicalItemCount\", \"accountAge\",\n",
        "                  \"paymentInstrumentAgeInAccount\", \"numPaymentRejects1dPerUser\"\n",
        "                 ]\n",
        "\n",
        "categorical_features=[\"transactionCurrencyCode\", \"browserLanguage\", \"paymentInstrumentType\", \"cardType\", \"cvvVerifyResult\",\n",
        "                      \"isUserRegistered\"\n",
        "                     ]                           \n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('cleaner', NumericCleaner())\n",
        "])\n",
        "                               \n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('cleaner', CategoricalCleaner()),\n",
        "    ('encoder', OrdinalEncoder())])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411436751
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the transformation pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_result = preprocessor.fit_transform(tagged_df_clean)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411439914
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(preprocessed_result).info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411440011
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model\n",
        "\n",
        "With all the hard work of preparing the data behind you, you are now ready to train the model. In this case you will train a decision tree based ensemble model `GradientBoostingClassifier`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "X = preprocessed_result\n",
        "y = tagged_df_clean['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "gbct = GradientBoostingClassifier()\n",
        "gbct.fit(X_train, y_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411459023
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now use the trained model to make predictions against the test set and evaluate the performance."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_preds = gbct.predict(X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411459123
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "confusion_matrix(y_test, y_test_preds)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411459235
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test save and load of the model\n",
        "\n",
        "When batch scoring, you will typically work with a model that has been saved off to a shared location. That way, the jobs that use the model for batch processing can easily retrieve the latest version of the model. A good practice is to version that model in Azure Machine Learning service first by registering it. Then any jobs can retrieve the model from the Azure Machine Learning service registry.\n",
        "\n",
        "Step through the following cells to create some helper functions to prepare for this."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azureml\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.model import Model\n",
        "\n",
        "# sklearn.externals.joblib was deprecated in 0.21\n",
        "from sklearn import __version__ as sklearnver\n",
        "from packaging.version import Version\n",
        "if Version(sklearnver) < Version(\"0.21.0\"):\n",
        "    from sklearn.externals import joblib\n",
        "else:\n",
        "    import joblib"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411459447
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def saveModelToAML(ws, model, model_folder_path=\"models\", model_name=\"batch-score\"):\n",
        "    # create the models subfolder if it does not exist in the current working directory\n",
        "    target_dir = './' + model_folder_path\n",
        "    if not os.path.exists(target_dir):\n",
        "        os.makedirs(target_dir)\n",
        "  \n",
        "    # save the model to disk\n",
        "    joblib.dump(model, model_folder_path + '/' + model_name + '.pkl')\n",
        "  \n",
        "    # notice for the model_path, we supply the name of the model outputs folder without a trailing slash\n",
        "    # anything present in the model folder path will be uploaded to AML along with the model\n",
        "    print(\"Registering and uploading model...\")\n",
        "    registered_model = Model.register(model_path=model_folder_path, \n",
        "                                      model_name=model_name, \n",
        "                                      workspace=ws)\n",
        "    return registered_model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411459569
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadModelFromAML(ws, model_name=\"batch-score\"):\n",
        "  # download the model folder from AML to the current working directory\n",
        "  model_file_path = Model.get_model_path(model_name, _workspace=ws)\n",
        "  print('Loading model from:', model_file_path)\n",
        "  model = joblib.load(model_file_path)\n",
        "  return model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411459668
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model to Azure Machine Learning service."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model to the AML Workspace\n",
        "registeredModel = saveModelToAML(ws, gbct)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411461070
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, try out the loading process by getting the model from Azure Machine Learning service, loading the model and then using the model for scoring."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Test loading the model\n",
        "\n",
        "gbct = loadModelFromAML(ws)\n",
        "y_test_preds = gbct.predict(X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411461738
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_preds"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411461847
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"subscription_id = '\" + ws.subscription_id + \"'\",\n",
        "      \"resource_group = '\" + ws.resource_group + \"'\",\n",
        "      \"workspace_name = '\" + ws.name + \"'\",\n",
        "      \"workspace_region = '\" + ws.location + \"'\", sep='\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613411461939
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Important**: Copy the output of the cell above and paste it to Notepad or similar text editor for later."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next\n",
        "\n",
        "Congratulations, you have completed Exercise 3.\n",
        "\n",
        "After you have **copied the output of the cell above**, that contains connection information to your Azure ML workspace, please return to the Cosmos DB real-time advanced analytics hands-on lab setup guide and continue on to Exercise 4."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}