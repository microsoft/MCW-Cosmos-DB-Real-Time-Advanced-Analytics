{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the real-time scoring model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The team at Woodgrove Bank has provided you with exported CSV copies of historical data for you to train your model against. Run the following cell to load required libraries and download the data sets from the Azure ML datastore."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade azureml-train-automl-runtime\r\n",
        "!pip install --upgrade azureml-automl-runtime\r\n",
        "!pip install --upgrade scikit-learn\r\n",
        "!pip install --upgrade numpy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Environment, Datastore, Dataset\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.core.run import Run\n",
        "from azureml.core.model import Model\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "# sklearn.externals.joblib was deprecated in 0.21\n",
        "from sklearn import __version__ as sklearnver\n",
        "from packaging.version import Version\n",
        "if Version(sklearnver) < Version(\"0.21.0\"):\n",
        "    from sklearn.externals import joblib\n",
        "else:\n",
        "    import joblib\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "# Load data\n",
        "ds = Datastore.get(ws, \"woodgrovestorage\")\n",
        "\n",
        "account_ds = Dataset.Tabular.from_delimited_files(path = [(ds, 'synapse/Account_Info.csv')])\n",
        "fraud_ds = Dataset.Tabular.from_delimited_files(path = [(ds, 'synapse/Fraud_Transactions.csv')])\n",
        "untagged_ds = Dataset.Tabular.from_delimited_files(path = [(ds, 'synapse/Untagged_Transactions.csv')])\n",
        "\n",
        "# Create pandas dataframes from datasets\n",
        "account_df = account_ds.to_pandas_dataframe()\n",
        "fraud_df = fraud_ds.to_pandas_dataframe()\n",
        "untagged_df = untagged_ds.to_pandas_dataframe()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511718401
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "View the fraud dataframe. \n",
        "\n",
        "NOTE: The schema documentation for the data used here is available at:\n",
        "https://microsoft.github.io/r-server-fraud-detection/input_data.html"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511718572
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "View the account info dataframe."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511718776
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "View the untagged transactions dataframe."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "###### Reorder the column of dataframe by ascending order in pandas \n",
        "cols=untagged_df.columns.tolist()\n",
        "cols.sort()\n",
        "untagged_df=untagged_df[cols]\n",
        "\n",
        "untagged_df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511718973
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data\n",
        "\n",
        "The raw data has some issues we need to cleanup before we can use it to train a model, which we perform in the following cells."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare accounts\n",
        "\n",
        "Begin by cleaning the data in accounts data set.\n",
        "Remove columns that have very few or no values: `accountOwnerName`, `accountAddress`, `accountCity` and `accountOpenDate` "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean = account_df[[\"accountID\", \"transactionDate\", \"transactionTime\", \n",
        "                               \"accountPostalCode\", \"accountState\", \"accountCountry\", \n",
        "                               \"accountAge\", \"isUserRegistered\", \"paymentInstrumentAgeInAccount\", \n",
        "                               \"numPaymentRejects1dPerUser\"]]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511719101
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a copy of the dataframe so our data manipulation does not affect the original."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean = account_df_clean.copy()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511719240
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ensure that values that are not numeric (e.g., they have incorrect string values or garbage data) are converted to NaN and then we can fill those NaN values with 0."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean['paymentInstrumentAgeInAccount'] = pd.to_numeric(account_df_clean['paymentInstrumentAgeInAccount'], errors='coerce')\n",
        "account_df_clean['paymentInstrumentAgeInAccount'] = account_df_clean[['paymentInstrumentAgeInAccount']].fillna(0)['paymentInstrumentAgeInAccount']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511719390
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's convert the `numPaymentRejects1dPerUser` so that the column has a datatype of `float` instead of `object`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean[\"numPaymentRejects1dPerUser\"] = account_df_clean[[\"numPaymentRejects1dPerUser\"]].astype(float)[\"numPaymentRejects1dPerUser\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511719499
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the results of our cleanup of this one column. Looks like the most payment declines/rejects that happen to a given user in one day happens either zero times or 1 time, and then trails off quickly. After 5 times the number of rejects per user per day is down to 136."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_df_clean[\"numPaymentRejects1dPerUser\"].value_counts()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511719630
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`account_df_clean` is now ready for use in modeling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare untagged transactions\n",
        "\n",
        "Next, cleanup the untagged transactions data set. There are 16 columns in the untagged_transactions whose values are all null, let's drop these columns to simplify our dataset."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean = untagged_df.dropna(axis=1, how=\"all\").copy()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511719849
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can examine the count of non-null values, and view the inferred data type for each column by running the following cell. Looking at the output of the cell, we have some work to do. For a start, we have columns with fewer than 200,000 non-null values. This means there are some null values in that column that we need to fix."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511719995
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's cleanup the `localHour` field. \n",
        "\n",
        "Replace null values in `localHour` with `-99`. Also replace values of `-1` with `-99`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean[\"localHour\"] = untagged_df_clean[\"localHour\"].fillna(-99)\n",
        "untagged_df_clean.loc[untagged_df_clean.loc[:,\"localHour\"] == -1, \"localHour\"] = -99"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511720100
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm the values now look good."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean[\"localHour\"].value_counts()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511720219
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean up the remaining null fields:\n",
        "- Fix missing values for location fields by setting them to `NA` for unknown. \n",
        "- Set `isProxyIP` to False\n",
        "- Set `cardType` to `U` for unknown (which is a new level)\n",
        "- Set `cvvVerifyResult` to `N` which means for those where the transaction failed because the wrong CVV2 number was entered ro no CVV2 numebr was entered, treat those as if there was no CVV2 match."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean = untagged_df_clean.fillna(value={\"ipState\": \"NA\", \"ipPostcode\": \"NA\", \"ipCountryCode\": \"NA\", \n",
        "                               \"isProxyIP\":False, \"cardType\": \"U\", \n",
        "                               \"paymentBillingPostalCode\" : \"NA\", \"paymentBillingState\":\"NA\",\n",
        "                               \"paymentBillingCountryCode\" : \"NA\", \"cvvVerifyResult\": \"N\"\n",
        "                              })"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511720330
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm all null values have been addressed."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_df_clean.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511720515
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `transactionScenario` column provides no insights because all rows have the same `A` value. Let's drop that column. Same idea for the `transactionType` column."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "del untagged_df_clean[\"transactionScenario\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511720628
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del untagged_df_clean[\"transactionType\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511720733
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`untagged_df_clean` is now ready for use in modeling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare fraud transactions\n",
        "\n",
        "Now move on to preparing the fraud transactions data set.\n",
        "\n",
        "The `transactionDeviceId` has no meaningful values, so we will drop it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df_clean = fraud_df.copy()\n",
        "del fraud_df_clean['transactionDeviceId']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511720840
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fraud data set has a `localHour` field that we need to fill missing values, just as we did for the account data set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df_clean[\"localHour\"] = fraud_df_clean[\"localHour\"].fillna(-99)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511721027
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Examine your work, you should have 8640 non-null values in each column."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "fraud_df_clean.info()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511721139
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`fraud_df_clean` is now ready for use in modeling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create labels\n",
        "\n",
        "The goal is to create a dataframe with all transactions, where each transaction is tagged via the `isFraud` column with a value of `0` - no fraud or `1` - fraudulent. \n",
        "\n",
        "Any transactions that appear in untagged_transactions dataframe that also appear in the fraud dataframe will be marked as fraudulent. \n",
        "\n",
        "The remaining transactions will be marked as not fraudulent. \n",
        "\n",
        "Run the following cells to create the labels series."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "all_labels = untagged_df_clean[\"transactionID\"].isin(fraud_df_clean[\"transactionID\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511721246
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_transactions = untagged_df_clean"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511721358
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create feature engineering pipeline\n",
        "\n",
        "In the following cells we will define two custom estimators that will be used in pipeline to prepare the data.\n",
        "\n",
        "We collect these estimators in a module and then save this module file in the models directory. Then we will use the classes in this module during both model training and model scoring. During deployment, when you register the model using `Model.register` (as we do later in `deployModelAsWebService`), all files in the models directory are uploaded with the model.\n",
        "\n",
        "The module containing the estimators used to transform the data before scoring needs to be deployed with the model. This ensures that the code can be executed in the context of the webservice, when the serialized pipeline is loaded and used for scoring. \n",
        "\n",
        "First we need to create the models directory if it does not already exist."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "import os\n",
        "\n",
        "# Create a temporary folder to store locally relevant content for this notebook\n",
        "tempFolderName = 'FileStore/mcw_cdb_{0}'.format(uuid.uuid4())\n",
        "\n",
        "models_dir = tempFolderName + \"/models/\"\n",
        "if not os.path.exists(models_dir):\n",
        "    os.makedirs(models_dir)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511721463
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can save our estimators module."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# write out to models/customestimators.py\n",
        "scoring_service = \"\"\"\n",
        "import pandas as pd\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "class NumericCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self = self\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"NumericCleaner.fit called\")\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"NumericCleaner.transform called\")\n",
        "        X[\"localHour\"] = X[\"localHour\"].fillna(-99)\n",
        "        X.loc[X.loc[:,\"localHour\"] == -1, \"localHour\"] = -99\n",
        "        return X\n",
        "\n",
        "class CategoricalCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self = self\n",
        "    def fit(self, X, y=None):\n",
        "        print(\"CategoricalCleaner.fit called\")\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        print(\"CategoricalCleaner.transform called\")\n",
        "        X = X.fillna(value={\"cardType\":\"U\",\"cvvVerifyResult\": \"N\"})\n",
        "        return X\n",
        "\"\"\" \n",
        "\n",
        "with open(models_dir + \"customestimators.py\", \"w\") as file:\n",
        "    file.write(scoring_service)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511721585
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You need to add the created module to our python search path so it can be found and loaded in this notebook environment:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from os.path import dirname\n",
        "sys.path.append(models_dir)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511721714
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load the estimators."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from customestimators import NumericCleaner, CategoricalCleaner"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511721824
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now build the pipeline that will prepare the data. \n",
        "\n",
        "The gist of the following cell is to split the data preparation into two paths, splitting the data sets vertically, and then combine the result. The `ColumnTransformer` will effectively concatenate the data frame that results from the numeric transformations with the data frame resulting from the categorical transformations. \n",
        "\n",
        "- Numeric Transformer Pipeline: We use the custom transformers created previously to cleanup the numeric columns. Since the model you will train in this notebook is a Support Vector Machine classifier, we need to standardize the scale of numeric values which is what the `StandardScaler` provides.\n",
        "- Categorical Transformer Pipeline: We use the custome transformer created previously cleanup the categorical columns. Then we one-hot encode each value of each categorical column, resulting in a wider data frame with one column for each possible value (and 1 appearing in rows that had that value)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "numeric_features=[\"transactionAmountUSD\", \"transactionDate\", \"transactionTime\", \"localHour\", \n",
        "                  \"transactionIPaddress\", \"digitalItemCount\", \"physicalItemCount\"]\n",
        "\n",
        "categorical_features=[\"transactionCurrencyCode\", \"browserLanguage\", \"paymentInstrumentType\", \"cardType\", \"cvvVerifyResult\"]                           \n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('cleaner', NumericCleaner()),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "                               \n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('cleaner', CategoricalCleaner()),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511721941
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511722099
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's confirm we run all our historical data thru this transformation pipeline and observe the resulting shape."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_result = preprocessor.fit_transform(all_transactions)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511722317
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_result.shape"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511722445
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(preprocessed_result.todense())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511722575
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create pipeline and train a simple model\n",
        "\n",
        "Now you will build upon the transformation pipeline you created previously to train a model to classify rows as fraudulent or not fraudulent.\n",
        "\n",
        "Run the following cells to make sure you've imported the dependencies for the pipeline (you probably already have, but having them clearly loaded here will help you when porting your code to a web service)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from customestimators import NumericCleaner, CategoricalCleaner\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511722677
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As might be obvious, our data has a lot of samples that are not fraudulent. If we proceed to train a model, we will effectively train the model to predict non-fraud. This situation where one class (non-fraud) appears much more often than the others (fraud) is called a class imbalance, and to mitigate its effect we can reduce the number of non-fraud samples so that we have the same number of non-fraud and fraud samples. \n",
        "\n",
        "Run the following cells to downsize and then randomly sample 1,151 non-fraud rows, and then we'll union these row with our 1,151 fraud rows.\n",
        "\n",
        "> Feel free to ignore any `SettingWithCopyWarning` warnings in the cell output below."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "only_fraud_samples = all_transactions.loc[all_labels == True]\n",
        "only_fraud_samples[\"label\"] = True\n",
        "only_non_fraud_samples = all_transactions.loc[all_labels == False]\n",
        "only_non_fraud_samples[\"label\"] = False\n",
        "random_non_fraud_samples = only_non_fraud_samples.sample(n=1151, replace=False, random_state=42)\n",
        "balanced_transactions = random_non_fraud_samples.append(only_fraud_samples)\n",
        "\n",
        "balanced_transactions[\"label\"].value_counts()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511722943
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you need to separate out the label column from the dataframe so the labels are not used as input features:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_labels = balanced_transactions[\"label\"]\n",
        "del balanced_transactions[\"label\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511723055
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you will create subsets of the training data frame, one that will be used for training the model `X_train` and `y_train` and the another that reserved for testing its performance `X_test` and `y_test`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(balanced_transactions, balanced_labels, \n",
        "                                                    test_size=0.2, random_state=42)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511723155
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now train the model. In this case, you will use the `LinearSVC` class.\n",
        "\n",
        "> Feel free to ignore any `ConvergenceWarning` warnings in the cell output below"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm_clf = Pipeline((\n",
        "    (\"preprocess\", preprocessor),\n",
        "    (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))\n",
        "))\n",
        "svm_clf.fit(X_train, y_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511723343
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model predicting against a single row from the test set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "svm_clf.predict(X_test[0:1])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511723457
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, evaluate the model by examining how well it is predicting against all data in the training set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_preds = svm_clf.predict(X_train)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511723564
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a confusion matrix to see how your model performed when correctly predicting non-fraud and fraud (the top left and bottom right values). Also, examine how your model made mistakes (the bottom left and top right values). In the below, the column headers are predicted non-fraud and predicted fraud, and the row headers are actually non-fraud, and actually fraud (e.g., as described by the training data)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "confusion_matrix(y_train, y_train_preds)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511723681
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the performance of your model using the common set of metrics for a classifier. Do you think this is good or bad?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_train, y_train_preds))\n",
        "print(\"Precision:\", precision_score(y_train, y_train_preds))\n",
        "print(\"Recall:\", recall_score(y_train, y_train_preds))\n",
        "print(\"F1:\", f1_score(y_train, y_train_preds))\n",
        "print(\"AUC:\", roc_auc_score(y_train, y_train_preds))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511723874
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given that this is just a parsimonous model, this model provides a start that performs better than random (as indicated by the AUC being greater than 0.5). There is more work (such as additional feature engineering) that can be done to improve this beyond the current performance that you would want to do before deploying it in production, but that is out of scope for this lab. A parsiminous model helps us to both see if the desired classification is possible given the data and allows to quickly get to something we can deploy as a service to enable integration early on. Then we can iterate deploying improved versions of the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, evaluate the same using the test data set, using data the trained model has not seen. How does it perform?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_preds = svm_clf.predict(X_test)\n",
        "print(confusion_matrix(y_test, y_test_preds))\n",
        "print(accuracy_score(y_test, y_test_preds))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_test_preds))\n",
        "print(\"Precision:\", precision_score(y_test, y_test_preds))\n",
        "print(\"Recall:\", recall_score(y_test, y_test_preds))\n",
        "print(\"F1:\", f1_score(y_test, y_test_preds))\n",
        "print(\"AUC:\", roc_auc_score(y_test, y_test_preds))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511723990
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The overall performance of the model against data it has not seen (the test data) is similar to how it performs with the training data. That's a good sign, indicating we did not overfit the model to the training data.\n",
        "\n",
        "Next, let's look the steps to prepare the model for deployment as a web service."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model to disk\n",
        "\n",
        "In preparation for deploying the model, you need to save the model to disk."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(svm_clf, models_dir + 'fraud_score.pkl')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511724126
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test loading the model\n",
        "\n",
        "Next simulate re-loading the model from disk, just like the web service (which you will create in a moment) will have to do."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from customestimators import NumericCleaner, CategoricalCleaner\n",
        "\n",
        "# sklearn.externals.joblib was deprecated in 0.21\n",
        "from sklearn import __version__ as sklearnver\n",
        "from packaging.version import Version\n",
        "if Version(sklearnver) < Version(\"0.21.0\"):\n",
        "    from sklearn.externals import joblib\n",
        "else:\n",
        "    import joblib\n",
        "\n",
        "desired_cols = ['accountID',\n",
        " 'browserLanguage',\n",
        " 'cardType',\n",
        " 'cvvVerifyResult',\n",
        " 'digitalItemCount',\n",
        " 'ipCountryCode',\n",
        " 'ipPostcode',\n",
        " 'ipState',\n",
        " 'isProxyIP',\n",
        " 'localHour',\n",
        " 'paymentBillingCountryCode',\n",
        " 'paymentBillingPostalCode',\n",
        " 'paymentBillingState',\n",
        " 'paymentInstrumentType',\n",
        " 'physicalItemCount',\n",
        " 'transactionAmount',\n",
        " 'transactionAmountUSD',\n",
        " 'transactionCurrencyCode',\n",
        " 'transactionDate',\n",
        " 'transactionID',\n",
        " 'transactionIPaddress',\n",
        " 'transactionTime']\n",
        "\n",
        "scoring_pipeline = joblib.load(models_dir + 'fraud_score.pkl')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511724254
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "untagged_ds = Dataset.Tabular.from_delimited_files(path = [(ds, 'synapse/Untagged_Transactions.csv')])\n",
        "untagged_df_fresh = untagged_ds.to_pandas_dataframe()\n",
        "untagged_df_fresh=untagged_df_fresh[desired_cols]\n",
        "\n",
        "test_pipeline_preds = scoring_pipeline.predict(untagged_df_fresh)\n",
        "test_pipeline_preds"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511724553
        },
        "scrolled": true
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_row = untagged_df_fresh.iloc[:1]\n",
        "test_pipeline_preds2 = scoring_pipeline.predict(one_row)\n",
        "test_pipeline_preds2"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511724654
        },
        "scrolled": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the model in the Azure ML workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml\n",
        "from azureml.core import Workspace, Webservice\n",
        "from azureml.core.model import Model\n",
        "from azureml.exceptions import WebserviceException\n",
        "from azureml.core.resource_configuration import ResourceConfiguration"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511724767
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sklearnver)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511724970
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Register the model, providing details on the framework that was used to create the model. The cell output above shows the version of scikit-learn we are using. Additionally, we specify the desired resources (CPU and Memory) to be allocated for the deployment of the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.resource_configuration import ResourceConfiguration\n",
        "\n",
        "# Register the model with the workspace\n",
        "registered_model = Model.register(model_path=models_dir, model_name=\"fraud-score\", workspace=ws, model_framework=Model.Framework.SCIKITLEARN, model_framework_version=sklearnver, resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5), description='Fraud Detection Model')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511726973
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Note**: Please note that executing the next few cells can take between **7** and **10** minutes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy the model to Azure Container Instances. Once deployment is complete - you will see the message **ACI Service creation operation finished, operation \"Succeeded\"**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "service_name = \"scoringservice\"\n",
        "\n",
        "# delete any existing service with the same name\n",
        "try:\n",
        "  Webservice(ws, service_name).delete()\n",
        "except WebserviceException:\n",
        "  pass\n",
        "\n",
        "# deploy the registered model to Azure Container Instances\n",
        "service = Model.deploy(ws, service_name, [registered_model])\n",
        "service.wait_for_deployment(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511903507
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, test your deployed web service."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# test the web service\n",
        "import json\n",
        "untagged_ds = Dataset.Tabular.from_delimited_files(path = [(ds, 'synapse/Untagged_Transactions.csv')])\n",
        "untagged_df_fresh = untagged_ds.to_pandas_dataframe()\n",
        "untagged_df_fresh=untagged_df_fresh[desired_cols]\n",
        "input_df = untagged_df_fresh.iloc[:5]\n",
        "# Convert dataframe to JSON, setting index=False so we don't add the index column\n",
        "json_df = input_df.to_json(orient='table', index=False)\n",
        "result = service.run(input_data=json_df)\n",
        "result\n",
        "\n",
        "# Uncomment the line below to output the JSON data for testing the service endpoint from CURL or other external application\n",
        "# json.dumps(json_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1613511909190
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}